// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.10
// 	protoc        (unknown)
// source: xai/api/v1/sample.proto

package v1

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	timestamppb "google.golang.org/protobuf/types/known/timestamppb"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Reasons why the model stopped sampling.
type FinishReason int32

const (
	// Invalid reason.
	FinishReason_REASON_INVALID FinishReason = 0
	// The max_len parameter specified on the input is reached.
	FinishReason_REASON_MAX_LEN FinishReason = 1
	// The maximum context length of the model is reached.
	FinishReason_REASON_MAX_CONTEXT FinishReason = 2
	// One of the stop words was found.
	FinishReason_REASON_STOP FinishReason = 3
	// A tool call is included in the response.
	FinishReason_REASON_TOOL_CALLS FinishReason = 4
	// Time limit has been reached.
	FinishReason_REASON_TIME_LIMIT FinishReason = 5
)

// Enum value maps for FinishReason.
var (
	FinishReason_name = map[int32]string{
		0: "REASON_INVALID",
		1: "REASON_MAX_LEN",
		2: "REASON_MAX_CONTEXT",
		3: "REASON_STOP",
		4: "REASON_TOOL_CALLS",
		5: "REASON_TIME_LIMIT",
	}
	FinishReason_value = map[string]int32{
		"REASON_INVALID":     0,
		"REASON_MAX_LEN":     1,
		"REASON_MAX_CONTEXT": 2,
		"REASON_STOP":        3,
		"REASON_TOOL_CALLS":  4,
		"REASON_TIME_LIMIT":  5,
	}
)

func (x FinishReason) Enum() *FinishReason {
	p := new(FinishReason)
	*p = x
	return p
}

func (x FinishReason) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (FinishReason) Descriptor() protoreflect.EnumDescriptor {
	return file_xai_api_v1_sample_proto_enumTypes[0].Descriptor()
}

func (FinishReason) Type() protoreflect.EnumType {
	return &file_xai_api_v1_sample_proto_enumTypes[0]
}

func (x FinishReason) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use FinishReason.Descriptor instead.
func (FinishReason) EnumDescriptor() ([]byte, []int) {
	return file_xai_api_v1_sample_proto_rawDescGZIP(), []int{0}
}

// Request to get a text completion response sampling.
type SampleTextRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Text prompts to sample on.
	Prompt []string `protobuf:"bytes,1,rep,name=prompt,proto3" json:"prompt,omitempty"`
	// Name or alias of the model to be used.
	Model string `protobuf:"bytes,3,opt,name=model,proto3" json:"model,omitempty"`
	// The number of completions to create concurrently. A single completion will
	// be generated if the parameter is unset. Each completion is charged at the
	// same rate. You can generate at most 128 concurrent completions.
	N *int32 `protobuf:"varint,8,opt,name=n,proto3,oneof" json:"n,omitempty"`
	// The maximum number of tokens to sample. If unset, the model samples until
	// one of the following stop-conditions is reached:
	// - The context length of the model is exceeded
	// - One of the `stop` sequences has been observed.
	//
	// We recommend choosing a reasonable value to reduce the risk of accidental
	// long-generations that consume many tokens.
	MaxTokens *int32 `protobuf:"varint,7,opt,name=max_tokens,json=maxTokens,proto3,oneof" json:"max_tokens,omitempty"`
	// A random seed used to make the sampling process deterministic. This is
	// provided in a best-effort basis without guarantee that sampling is 100%
	// deterministic given a seed. This is primarily provided for short-lived
	// testing purposes. Given a fixed request and seed, the answers may change
	// over time as our systems evolve.
	Seed *int32 `protobuf:"varint,11,opt,name=seed,proto3,oneof" json:"seed,omitempty"`
	// String patterns that will cause the sampling procedure to stop prematurely
	// when observed.
	// Note that the completion is based on individual tokens and sampling can
	// only terminate at token boundaries. If a stop string is a substring of an
	// individual token, the completion will include the entire token, which
	// extends beyond the stop string.
	// For example, if `stop = ["wor"]` and we prompt the model with "hello" to
	// which it responds with "world", then the sampling procedure will stop after
	// observing the "world" token and the completion will contain
	// the entire world "world" even though the stop string was just "wor".
	// You can provide at most 8 stop strings.
	Stop []string `protobuf:"bytes,12,rep,name=stop,proto3" json:"stop,omitempty"`
	// A number between 0 and 2 used to control the variance of completions.
	// The smaller the value, the more deterministic the model will become. For
	// example, if we sample 1000 answers to the same prompt at a temperature of
	// 0.001, then most of the 1000 answers will be identical. Conversely, if we
	// conduct the same experiment at a temperature of 2, virtually no two answers
	// will be identical. Note that increasing the temperature will cause
	// the model to hallucinate more strongly.
	Temperature *float32 `protobuf:"fixed32,14,opt,name=temperature,proto3,oneof" json:"temperature,omitempty"`
	// A number between 0 and 1 controlling the likelihood of the model to use
	// less-common answers. Recall that the model produces a probability for
	// each token. This means, for any choice of token there are thousands of
	// possibilities to choose from. This parameter controls the "nucleus sampling
	// algorithm". Instead of considering every possible token at every step, we
	// only look at the K tokens who's probabilities exceed `top_p`.
	// For example, if we set `top_p = 0.9`, then the set of tokens we actually
	// sample from, will have a probability mass of at least 90%. In practice,
	// low values will make the model more deterministic.
	TopP *float32 `protobuf:"fixed32,15,opt,name=top_p,json=topP,proto3,oneof" json:"top_p,omitempty"`
	// Number between -2.0 and 2.0.
	// Positive values penalize new tokens based on their existing frequency in the text so far,
	// decreasing the model's likelihood to repeat the same line verbatim.
	FrequencyPenalty *float32 `protobuf:"fixed32,13,opt,name=frequency_penalty,json=frequencyPenalty,proto3,oneof" json:"frequency_penalty,omitempty"`
	// Whether to return log probabilities of the output tokens or not.
	// If true, returns the log probabilities of each output token returned in the content of message.
	Logprobs bool `protobuf:"varint,5,opt,name=logprobs,proto3" json:"logprobs,omitempty"`
	// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
	// Not supported by grok-3 models.
	PresencePenalty *float32 `protobuf:"fixed32,9,opt,name=presence_penalty,json=presencePenalty,proto3,oneof" json:"presence_penalty,omitempty"`
	// An integer between 0 and 8 specifying the number of most likely tokens to return at each token position,
	// each with an associated log probability.
	// logprobs must be set to true if this parameter is used.
	TopLogprobs *int32 `protobuf:"varint,6,opt,name=top_logprobs,json=topLogprobs,proto3,oneof" json:"top_logprobs,omitempty"`
	// An opaque string supplied by the API client (customer) to identify a user.
	// The string will be stored in the logs and can be used in customer service
	// requests to identify certain requests.
	User          string `protobuf:"bytes,17,opt,name=user,proto3" json:"user,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SampleTextRequest) Reset() {
	*x = SampleTextRequest{}
	mi := &file_xai_api_v1_sample_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SampleTextRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SampleTextRequest) ProtoMessage() {}

func (x *SampleTextRequest) ProtoReflect() protoreflect.Message {
	mi := &file_xai_api_v1_sample_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SampleTextRequest.ProtoReflect.Descriptor instead.
func (*SampleTextRequest) Descriptor() ([]byte, []int) {
	return file_xai_api_v1_sample_proto_rawDescGZIP(), []int{0}
}

func (x *SampleTextRequest) GetPrompt() []string {
	if x != nil {
		return x.Prompt
	}
	return nil
}

func (x *SampleTextRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *SampleTextRequest) GetN() int32 {
	if x != nil && x.N != nil {
		return *x.N
	}
	return 0
}

func (x *SampleTextRequest) GetMaxTokens() int32 {
	if x != nil && x.MaxTokens != nil {
		return *x.MaxTokens
	}
	return 0
}

func (x *SampleTextRequest) GetSeed() int32 {
	if x != nil && x.Seed != nil {
		return *x.Seed
	}
	return 0
}

func (x *SampleTextRequest) GetStop() []string {
	if x != nil {
		return x.Stop
	}
	return nil
}

func (x *SampleTextRequest) GetTemperature() float32 {
	if x != nil && x.Temperature != nil {
		return *x.Temperature
	}
	return 0
}

func (x *SampleTextRequest) GetTopP() float32 {
	if x != nil && x.TopP != nil {
		return *x.TopP
	}
	return 0
}

func (x *SampleTextRequest) GetFrequencyPenalty() float32 {
	if x != nil && x.FrequencyPenalty != nil {
		return *x.FrequencyPenalty
	}
	return 0
}

func (x *SampleTextRequest) GetLogprobs() bool {
	if x != nil {
		return x.Logprobs
	}
	return false
}

func (x *SampleTextRequest) GetPresencePenalty() float32 {
	if x != nil && x.PresencePenalty != nil {
		return *x.PresencePenalty
	}
	return 0
}

func (x *SampleTextRequest) GetTopLogprobs() int32 {
	if x != nil && x.TopLogprobs != nil {
		return *x.TopLogprobs
	}
	return 0
}

func (x *SampleTextRequest) GetUser() string {
	if x != nil {
		return x.User
	}
	return ""
}

// Response of a text completion response sampling.
type SampleTextResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The ID of this request. This ID will also show up on your billing records
	// and you can use it when contacting us regarding a specific request.
	Id string `protobuf:"bytes,1,opt,name=id,proto3" json:"id,omitempty"`
	// Completions in response to the input messages. The number of completions is
	// controlled via the `n` parameter on the request.
	Choices []*SampleChoice `protobuf:"bytes,2,rep,name=choices,proto3" json:"choices,omitempty"`
	// A UNIX timestamp (UTC) indicating when the response object was created.
	// The timestamp is taken when the model starts generating response.
	Created *timestamppb.Timestamp `protobuf:"bytes,5,opt,name=created,proto3" json:"created,omitempty"`
	// The name of the model used for the request. This model name contains
	// the actual model name used rather than any aliases.
	// This means the this can be `grok-2-1212` even when the request was
	// specifying `grok-2-latest`.
	Model string `protobuf:"bytes,6,opt,name=model,proto3" json:"model,omitempty"`
	// Note supported yet. Included for compatibility reasons.
	SystemFingerprint string `protobuf:"bytes,7,opt,name=system_fingerprint,json=systemFingerprint,proto3" json:"system_fingerprint,omitempty"`
	// The number of tokens consumed by this request.
	Usage         *SamplingUsage `protobuf:"bytes,9,opt,name=usage,proto3" json:"usage,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SampleTextResponse) Reset() {
	*x = SampleTextResponse{}
	mi := &file_xai_api_v1_sample_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SampleTextResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SampleTextResponse) ProtoMessage() {}

func (x *SampleTextResponse) ProtoReflect() protoreflect.Message {
	mi := &file_xai_api_v1_sample_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SampleTextResponse.ProtoReflect.Descriptor instead.
func (*SampleTextResponse) Descriptor() ([]byte, []int) {
	return file_xai_api_v1_sample_proto_rawDescGZIP(), []int{1}
}

func (x *SampleTextResponse) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *SampleTextResponse) GetChoices() []*SampleChoice {
	if x != nil {
		return x.Choices
	}
	return nil
}

func (x *SampleTextResponse) GetCreated() *timestamppb.Timestamp {
	if x != nil {
		return x.Created
	}
	return nil
}

func (x *SampleTextResponse) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *SampleTextResponse) GetSystemFingerprint() string {
	if x != nil {
		return x.SystemFingerprint
	}
	return ""
}

func (x *SampleTextResponse) GetUsage() *SamplingUsage {
	if x != nil {
		return x.Usage
	}
	return nil
}

// Contains the response generated by the model.
type SampleChoice struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Indicating why the model stopped sampling.
	FinishReason FinishReason `protobuf:"varint,1,opt,name=finish_reason,json=finishReason,proto3,enum=xai_api.FinishReason" json:"finish_reason,omitempty"`
	// The index of this choice in the list of choices. If you set `n > 1` on
	// your request, you will receive more than one choice in your response.
	Index int32 `protobuf:"varint,2,opt,name=index,proto3" json:"index,omitempty"`
	// The actual text generated by the model.
	Text          string `protobuf:"bytes,3,opt,name=text,proto3" json:"text,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SampleChoice) Reset() {
	*x = SampleChoice{}
	mi := &file_xai_api_v1_sample_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SampleChoice) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SampleChoice) ProtoMessage() {}

func (x *SampleChoice) ProtoReflect() protoreflect.Message {
	mi := &file_xai_api_v1_sample_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SampleChoice.ProtoReflect.Descriptor instead.
func (*SampleChoice) Descriptor() ([]byte, []int) {
	return file_xai_api_v1_sample_proto_rawDescGZIP(), []int{2}
}

func (x *SampleChoice) GetFinishReason() FinishReason {
	if x != nil {
		return x.FinishReason
	}
	return FinishReason_REASON_INVALID
}

func (x *SampleChoice) GetIndex() int32 {
	if x != nil {
		return x.Index
	}
	return 0
}

func (x *SampleChoice) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

var File_xai_api_v1_sample_proto protoreflect.FileDescriptor

const file_xai_api_v1_sample_proto_rawDesc = "" +
	"\n" +
	"\x17xai/api/v1/sample.proto\x12\axai_api\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x16xai/api/v1/usage.proto\"\xac\x04\n" +
	"\x11SampleTextRequest\x12\x16\n" +
	"\x06prompt\x18\x01 \x03(\tR\x06prompt\x12\x14\n" +
	"\x05model\x18\x03 \x01(\tR\x05model\x12\x11\n" +
	"\x01n\x18\b \x01(\x05H\x00R\x01n\x88\x01\x01\x12\"\n" +
	"\n" +
	"max_tokens\x18\a \x01(\x05H\x01R\tmaxTokens\x88\x01\x01\x12\x17\n" +
	"\x04seed\x18\v \x01(\x05H\x02R\x04seed\x88\x01\x01\x12\x12\n" +
	"\x04stop\x18\f \x03(\tR\x04stop\x12%\n" +
	"\vtemperature\x18\x0e \x01(\x02H\x03R\vtemperature\x88\x01\x01\x12\x18\n" +
	"\x05top_p\x18\x0f \x01(\x02H\x04R\x04topP\x88\x01\x01\x120\n" +
	"\x11frequency_penalty\x18\r \x01(\x02H\x05R\x10frequencyPenalty\x88\x01\x01\x12\x1a\n" +
	"\blogprobs\x18\x05 \x01(\bR\blogprobs\x12.\n" +
	"\x10presence_penalty\x18\t \x01(\x02H\x06R\x0fpresencePenalty\x88\x01\x01\x12&\n" +
	"\ftop_logprobs\x18\x06 \x01(\x05H\aR\vtopLogprobs\x88\x01\x01\x12\x12\n" +
	"\x04user\x18\x11 \x01(\tR\x04userB\x04\n" +
	"\x02_nB\r\n" +
	"\v_max_tokensB\a\n" +
	"\x05_seedB\x0e\n" +
	"\f_temperatureB\b\n" +
	"\x06_top_pB\x14\n" +
	"\x12_frequency_penaltyB\x13\n" +
	"\x11_presence_penaltyB\x0f\n" +
	"\r_top_logprobsJ\x04\b\x02\x10\x03J\x04\b\x04\x10\x05J\x04\b\x10\x10\x11J\x04\b\x12\x10\x13\"\xfe\x01\n" +
	"\x12SampleTextResponse\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\tR\x02id\x12/\n" +
	"\achoices\x18\x02 \x03(\v2\x15.xai_api.SampleChoiceR\achoices\x124\n" +
	"\acreated\x18\x05 \x01(\v2\x1a.google.protobuf.TimestampR\acreated\x12\x14\n" +
	"\x05model\x18\x06 \x01(\tR\x05model\x12-\n" +
	"\x12system_fingerprint\x18\a \x01(\tR\x11systemFingerprint\x12,\n" +
	"\x05usage\x18\t \x01(\v2\x16.xai_api.SamplingUsageR\x05usage\"t\n" +
	"\fSampleChoice\x12:\n" +
	"\rfinish_reason\x18\x01 \x01(\x0e2\x15.xai_api.FinishReasonR\ffinishReason\x12\x14\n" +
	"\x05index\x18\x02 \x01(\x05R\x05index\x12\x12\n" +
	"\x04text\x18\x03 \x01(\tR\x04text*\x8d\x01\n" +
	"\fFinishReason\x12\x12\n" +
	"\x0eREASON_INVALID\x10\x00\x12\x12\n" +
	"\x0eREASON_MAX_LEN\x10\x01\x12\x16\n" +
	"\x12REASON_MAX_CONTEXT\x10\x02\x12\x0f\n" +
	"\vREASON_STOP\x10\x03\x12\x15\n" +
	"\x11REASON_TOOL_CALLS\x10\x04\x12\x15\n" +
	"\x11REASON_TIME_LIMIT\x10\x052\xa5\x01\n" +
	"\x06Sample\x12G\n" +
	"\n" +
	"SampleText\x12\x1a.xai_api.SampleTextRequest\x1a\x1b.xai_api.SampleTextResponse\"\x00\x12R\n" +
	"\x13SampleTextStreaming\x12\x1a.xai_api.SampleTextRequest\x1a\x1b.xai_api.SampleTextResponse\"\x000\x01B{\n" +
	"\vcom.xai_apiB\vSampleProtoP\x01Z'github.com/zchee/tumix/gollm/xai/api/v1\xa2\x02\x03XXX\xaa\x02\x06XaiApi\xca\x02\x06XaiApi\xe2\x02\x12XaiApi\\GPBMetadata\xea\x02\x06XaiApib\x06proto3"

var (
	file_xai_api_v1_sample_proto_rawDescOnce sync.Once
	file_xai_api_v1_sample_proto_rawDescData []byte
)

func file_xai_api_v1_sample_proto_rawDescGZIP() []byte {
	file_xai_api_v1_sample_proto_rawDescOnce.Do(func() {
		file_xai_api_v1_sample_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_xai_api_v1_sample_proto_rawDesc), len(file_xai_api_v1_sample_proto_rawDesc)))
	})
	return file_xai_api_v1_sample_proto_rawDescData
}

var file_xai_api_v1_sample_proto_enumTypes = make([]protoimpl.EnumInfo, 1)
var file_xai_api_v1_sample_proto_msgTypes = make([]protoimpl.MessageInfo, 3)
var file_xai_api_v1_sample_proto_goTypes = []any{
	(FinishReason)(0),             // 0: xai_api.FinishReason
	(*SampleTextRequest)(nil),     // 1: xai_api.SampleTextRequest
	(*SampleTextResponse)(nil),    // 2: xai_api.SampleTextResponse
	(*SampleChoice)(nil),          // 3: xai_api.SampleChoice
	(*timestamppb.Timestamp)(nil), // 4: google.protobuf.Timestamp
	(*SamplingUsage)(nil),         // 5: xai_api.SamplingUsage
}
var file_xai_api_v1_sample_proto_depIdxs = []int32{
	3, // 0: xai_api.SampleTextResponse.choices:type_name -> xai_api.SampleChoice
	4, // 1: xai_api.SampleTextResponse.created:type_name -> google.protobuf.Timestamp
	5, // 2: xai_api.SampleTextResponse.usage:type_name -> xai_api.SamplingUsage
	0, // 3: xai_api.SampleChoice.finish_reason:type_name -> xai_api.FinishReason
	1, // 4: xai_api.Sample.SampleText:input_type -> xai_api.SampleTextRequest
	1, // 5: xai_api.Sample.SampleTextStreaming:input_type -> xai_api.SampleTextRequest
	2, // 6: xai_api.Sample.SampleText:output_type -> xai_api.SampleTextResponse
	2, // 7: xai_api.Sample.SampleTextStreaming:output_type -> xai_api.SampleTextResponse
	6, // [6:8] is the sub-list for method output_type
	4, // [4:6] is the sub-list for method input_type
	4, // [4:4] is the sub-list for extension type_name
	4, // [4:4] is the sub-list for extension extendee
	0, // [0:4] is the sub-list for field type_name
}

func init() { file_xai_api_v1_sample_proto_init() }
func file_xai_api_v1_sample_proto_init() {
	if File_xai_api_v1_sample_proto != nil {
		return
	}
	file_xai_api_v1_usage_proto_init()
	file_xai_api_v1_sample_proto_msgTypes[0].OneofWrappers = []any{}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_xai_api_v1_sample_proto_rawDesc), len(file_xai_api_v1_sample_proto_rawDesc)),
			NumEnums:      1,
			NumMessages:   3,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_xai_api_v1_sample_proto_goTypes,
		DependencyIndexes: file_xai_api_v1_sample_proto_depIdxs,
		EnumInfos:         file_xai_api_v1_sample_proto_enumTypes,
		MessageInfos:      file_xai_api_v1_sample_proto_msgTypes,
	}.Build()
	File_xai_api_v1_sample_proto = out.File
	file_xai_api_v1_sample_proto_goTypes = nil
	file_xai_api_v1_sample_proto_depIdxs = nil
}
